
services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.gpu
    container_name: pmoves-dox-backend
    volumes:
      - ./backend/uploads:/app/uploads
      - ./backend/artifacts:/app/artifacts
      - ./backend/app:/app/app
      - ./backend/data:/app/data
      - hf-cache:/root/.cache/huggingface
    env_file:
      - path: ../pmoves/env.shared
        required: false
      - .env.local
    environment:
      # Override default values (env_file provides the actual values)
      # DB_BACKEND: Set in .env.local (sqlite or supabase), defaults to sqlite
      - DATABASE_URL=postgresql://postgres:postgres@supabase-db:5432/postgres
      - SUPABASE_URL=http://supabase-rest:3000
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - REDIS_URL=redis://redis:6379
      - NATS_URL=nats://nats:4222
      - FRONTEND_ORIGIN=http://localhost:8484,http://127.0.0.1:8484,http://frontend:3001
      - PORT=8484
      - OLLAMA_BASE_URL=http://ollama:11434
      - SEARCH_DEVICE=cpu
      - OPEN_PDF_ENABLED=true
      # Use TensorZero gateway as OpenAI-compatible endpoint
      - OPENAI_BASE_URL=http://tensorzero-gateway:3000/openai/v1
      - TENSORZERO_BASE_URL=http://tensorzero-gateway:3000
      - OPENAI_API_KEY=${OPENAI_API_KEY:-tensorzero}
      # TensorZero Embeddings (for SearchIndex - uses GPU Orchestrator)
      - TENSORZERO_EMBEDDING_MODEL=qwen3_embedding_8b_local
      # Neo4j Knowledge Graph - Local (DoX container)
      - NEO4J_LOCAL_URI=bolt://neo4j:7687
      - NEO4J_LOCAL_USER=neo4j
      # Neo4j Knowledge Graph - Parent (PMOVES.AI shared)
      - NEO4J_PARENT_URI=bolt://pmoves-neo4j-1:7687
      - NEO4J_PARENT_USER=neo4j
    ports:
      - "8484:8484"
    networks:
      - api_tier
      - app_tier
      - bus_tier
      - data_tier
      - pmoves_data  # Connect to parent PMOVES.AI services (TensorZero, Neo4j)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      - supabase-db
      - nats
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-qO", "-", "http://localhost:8484/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        NEXT_PUBLIC_API_BASE: /api
        NEXT_PUBLIC_NATS_WS_URL: ws://localhost:9223
    container_name: pmoves-dox-frontend
    env_file:
      - path: ../pmoves/env.shared
        required: false
      - .env.local
    environment:
      - API_URL=http://backend:8484
      - NEXT_PUBLIC_API_BASE=/api
      - NEXT_PUBLIC_NATS_WS_URL=ws://localhost:9223
      - PORT=3001
    ports:
      - "3001:3001"
    networks:
      - api_tier
    depends_on:
      - backend
    restart: unless-stopped

  nats:
    image: nats:latest
    container_name: pmoves-dox-nats
    command: -c /etc/nats/nats.conf
    ports:
      - "4223:4222"
      - "8223:8222"
      - "9223:9222"
    volumes:
      - ./backend/nats-config/nats.conf:/etc/nats/nats.conf
    networks:
      - bus_tier
      - api_tier
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-qO", "-", "http://localhost:8222/varz"]
      interval: 10s
      timeout: 5s
      retries: 5

  supabase-db:
    image: supabase/postgres:15.1.0.117
    container_name: supabase-db
    env_file:
      - path: ../pmoves/env.shared
        required: false
      - .env.local
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    networks:
      - data_tier
    volumes:
      - supabase_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  supabase-rest:
    image: postgrest/postgrest:v11.1.0
    container_name: supabase-rest
    env_file:
      - path: ../pmoves/env.shared
        required: false
      - .env.local
    environment:
      PGRST_DB_URI: postgres://postgres:postgres@supabase-db:5432/postgres
      PGRST_DB_SCHEMA: public,storage,graphql_public
      PGRST_DB_ANON_ROLE: anon
      # Serve API at /rest/v1 prefix for Supabase Python client compatibility
      PGRST_SERVER_PORT_PATH: /rest/v1
    ports:
      - "54321:3000"
    networks:
      - api_tier
      - data_tier
    depends_on:
      - supabase-db

  cipher-service:
    container_name: pmoves-dox-cipher
    build:
      context: ../PMOVES-BoTZ/features/cipher/pmoves_cipher
      dockerfile: Dockerfile
    # Internal service: No ports exposed to host. Reachable via http://cipher-service:3000 on app_tier
    networks:
      - app_tier
    environment:
      - CIPHER_API_PREFIX=""
      - NODE_ENV=production
      - OLLAMA_BASE_URL=http://ollama:11434
      - SEARCH_DEVICE=cpu
    volumes:
      - ../PMOVES-BoTZ/features/cipher/pmoves_cipher/memAgent/cipher.yml:/app/memAgent/cipher.yml
      - cipher-data:/app/.cipher
    depends_on:
      - ollama
      - nats
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-qO", "-", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  ollama:
    image: ollama/ollama:latest
    container_name: pmoves-dox-ollama-1
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - api_tier
      - app_tier
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  neo4j:
    image: neo4j:5.15-community
    container_name: pmoves-dox-neo4j
    environment:
      # Minimal Neo4j 5.x community config (no EULA needed for community)
      - NEO4J_AUTH=neo4j/pmovesNeo4jLocal2025
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=1G
      - NEO4J_dbms_memory_pagecache_size=512m
      - NEO4J_dbms_connector_bolt_advertised__address=:17687
      - NEO4J_dbms_connector_http_advertised__address=:17474
    ports:
      - "17474:7474"  # HTTP (different port to avoid conflict with parent)
      - "17687:7687"  # Bolt protocol
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
    networks:
      - data_tier
      - pmoves_data  # Connect to parent Neo4j for shared knowledge graphs
    healthcheck:
      # Healthcheck with hardcoded password matching NEO4J_AUTH
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "pmovesNeo4jLocal2025", "RETURN", "1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped


  glances:
    image: nicolargo/glances:latest-full
    container_name: pmoves-dox-glancer
    ports:
      - "61208:61208"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./external/Pmoves-Glancer/glances.conf:/glances/conf/glances.conf
    environment:
      - GLANCES_OPT=-C /glances/conf/glances.conf -w
      - TZ=America/New_York
    networks:
      - api_tier
      - app_tier
      - bus_tier
    restart: unless-stopped

  datavzrd:
    build:
      context: ./tools/datavzrd
      dockerfile: Dockerfile
    container_name: datavzrd
    profiles: ["tools"]
    environment:
      - VIZ_FILE=
    volumes:
      - ./backend/artifacts:/app/artifacts
    ports:
      - "5173:5173"
    networks:
      - app_tier
    depends_on:
      - backend
    restart: unless-stopped

  schemavzrd:
    build:
      context: ./tools/schemavzrd
      dockerfile: Dockerfile
    container_name: schemavzrd
    profiles: ["tools"]
    environment:
      - DB_URL=
      - OUTPUT_DIR=/app/out/schema
    volumes:
      - ./backend/artifacts:/app/out
    ports:
      - "5174:5174"
    networks:
      - app_tier
    restart: unless-stopped


  # Phase 9: Internal BoTZ Agents

  cipher:
    build:
      context: ./external/PMOVES-BotZ-gateway/sample-servers/mcp-proxy
    container_name: pmoves-botz-cipher
    environment:
      - MCP_COMMAND=python
      - MCP_ARGS=/app/cipher/app_memory.py
    ports:
      - "3025:8000"
    volumes:
      - ./external/PMOVES-BoTZ/features/cipher:/app/cipher
      - cipher-data:/data
    command: ["/usr/local/bin/fastmcp", "run", "src/main.py:app", "--transport", "streamable-http", "--port", "8000", "--host", "0.0.0.0"]
    networks:
      - app_tier
      - api_tier
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  postman-agent:
    build:
      context: ./external/PMOVES-BotZ-gateway/sample-servers/mcp-proxy
    container_name: pmoves-botz-postman
    env_file:
      - path: ../pmoves/env.shared
        required: false
      - .env.local
    environment:
      - MCP_COMMAND=npx
      - MCP_ARGS=@postman/postman-mcp-server@latest --full
      - POSTMAN_API_KEY=${POSTMAN_API_KEY}
    ports:
      - "3026:8000"
    networks:
      - app_tier
      - api_tier
    restart: unless-stopped

  docling:
    build:
      context: ../PMOVES-BoTZ/features/docling
      dockerfile: Dockerfile.docling-mcp
    container_name: pmoves-botz-docling
    ports:
      - "3020:8020"
    command: ["python", "docling_mcp_server.py", "--transport", "streamable-http", "--port", "8020", "--host", "0.0.0.0"]
    networks:
      - app_tier
      - api_tier
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8020/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  tensorzero:
    image: tensorzero/gateway:latest
    container_name: pmoves-botz-tensorzero
    command: ["--config-file", "/app/config/tensorzero.toml"]
    ports:
      - "3000:3000"
    volumes:
      - ./tensorzero.toml:/app/config/tensorzero.toml
    env_file:
      - path: ../pmoves/env.shared
        required: false
      - .env.local
    environment:
      - TENSORZERO_PROVIDER_OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - GOOGLE_AI_STUDIO_API_KEY=${GOOGLE_API_KEY}
      - TENSORZERO_CLICKHOUSE_URL=http://${CLICKHOUSE_USER:-tensorzero}:${CLICKHOUSE_PASSWORD:-tensorzero}@tensorzero-clickhouse:8123/default
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - app_tier
      - api_tier
      - pmoves_data  # Connect to parent PMOVES.AI for ClickHouse
    restart: unless-stopped

  agent-zero:
    build:
      context: ./external/PMOVES-Agent-Zero
      dockerfile: DockerfileLocal
    container_name: pmoves-agent-zero
    ports:
      - "50051:50051" # Agent Zero MCP/Web UI endpoint
    volumes:
      - ./external/PMOVES-Agent-Zero:/app
      - agent-zero-data:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - PROFILE=pmoves_custom
      # MCP Server Configuration (docking pattern)
      - MCP_SERVER_ENABLED=false
      - MCP_SERVER_TOKEN=
      # Standalone mode uses Web UI on WEB_UI_PORT (default 50051)
      # Docked mode exposes MCP at http://pmoves-agent-zero:50051/mcp/t-{token}/sse
      - WEB_UI_PORT=50051
      # TensorZero integration for LLM orchestration
      - TENSORZERO_API_BASE=http://tensorzero:3000/v1
    networks:
      - app_tier
      - api_tier
      - bus_tier
    depends_on:
      - tensorzero
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:50051/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  api_tier:
    driver: bridge
    name: pmoves_api
    ipam:
      config:
        - subnet: 172.30.1.0/24

  app_tier:
    driver: bridge
    name: pmoves_app
    internal: true
    ipam:
      config:
        - subnet: 172.30.2.0/24

  bus_tier:
    driver: bridge
    name: pmoves_bus
    internal: true
    ipam:
      config:
        - subnet: 172.30.3.0/24

  data_tier:
    driver: bridge
    name: pmoves_dox_data  # Renamed to avoid conflict with parent pmoves_data
    internal: true
    ipam:
      config:
        - subnet: 172.31.4.0/24  # Changed to avoid conflict with parent pmoves_data (172.30.4.0/24)

  monitoring_tier:
    driver: bridge
    name: pmoves_dox_monitoring  # Renamed to avoid conflict
    ipam:
      config:
        - subnet: 172.31.5.0/24  # Changed to avoid conflict with parent pmoves_monitoring (172.30.5.0/24)

  # External network to connect to parent PMOVES.AI services
  # (ClickHouse on pmoves_data, pmoves_monitoring)
  pmoves_data:
    external: true
    name: pmoves_data

volumes:
  hf-cache:
  ollama:
  cipher-data:
  supabase_data:
  nats_data:
  ollama_data:
  neo4j-data:
  neo4j-logs:
  agent-zero-data:
