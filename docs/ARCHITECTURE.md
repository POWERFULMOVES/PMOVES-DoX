# PMOVES-DoX Architecture

Technical architecture and system design documentation.

## Table of Contents

1. [System Overview](#system-overview)
2. [Architecture Diagram](#architecture-diagram)
3. [Backend Architecture](#backend-architecture)
4. [Frontend Architecture](#frontend-architecture)
5. [Data Flow](#data-flow)
6. [Processing Pipeline](#processing-pipeline)
7. [Storage Architecture](#storage-architecture)
8. [Search & Indexing](#search--indexing)
9. [Security Architecture](#security-architecture)
10. [Deployment Models](#deployment-models)

---

## System Overview

PMOVES-DoX is a document intelligence platform built on a modern microservices architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚â”€â”€â”€â”€â–¶â”‚   Next.js   â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI   â”‚
â”‚  (Client)   â”‚     â”‚  Frontend   â”‚     â”‚   Backend   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                    â”‚
                            â”‚              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                            â”‚              â”‚  SQLite   â”‚
                            â”‚              â”‚  Database â”‚
                            â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                    â”‚
                            â”‚              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                            â”‚              â”‚  FAISS    â”‚
                            â”‚              â”‚  Index    â”‚
                            â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                    â”‚
                            â”‚              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ datavzrd  â”‚
                                           â”‚ (Tools)   â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**Backend:**
- FastAPI (Python 3.10+)
- SQLModel/SQLAlchemy (ORM)
- Docling 2.x (PDF processing)
- FAISS (vector search)
- sentence-transformers (embeddings)
- spaCy (NER)
- LangExtract (tag extraction)

**Frontend:**
- Next.js 14 (React 18)
- TypeScript 5.3
- Tailwind CSS 3.4
- Axios (HTTP client)

**Infrastructure:**
- Docker & Docker Compose
- NVIDIA GPU support
- Jetson ARM64 support
- Optional Ollama integration

---

## Architecture Diagram

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Upload  â”‚  â”‚ Search  â”‚  â”‚   Q&A   â”‚  â”‚  Viz    â”‚      â”‚
â”‚  â”‚ UI      â”‚  â”‚ UI      â”‚  â”‚   UI    â”‚  â”‚  UI     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚           â”‚            â”‚            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   API Gateway       â”‚
         â”‚   (CORS, Routes)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Backend Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Ingestion   â”‚  â”‚ Analysis    â”‚  â”‚ Search      â”‚        â”‚
â”‚  â”‚ Pipeline    â”‚  â”‚ Engine      â”‚  â”‚ Engine      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                â”‚                 â”‚                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                          â”‚                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚         â”‚    Database Factory             â”‚                â”‚
â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                â”‚
â”‚         â”‚  â”‚SQLite  â”‚ or â”‚Supabase  â”‚    â”‚                â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Storage Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ Uploads  â”‚  â”‚Artifacts â”‚  â”‚  FAISS   â”‚                 â”‚
â”‚  â”‚ (Files)  â”‚  â”‚ (Output) â”‚  â”‚ (Index)  â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Backend Architecture

### FastAPI Application Structure

```python
# app/main.py - Entry point
from fastapi import FastAPI
from app.database_factory import init_database
from app.qa_engine import QAEngine
from app.search import SearchIndex

# Initialize services
db, DB_BACKEND_META = init_database()
qa_engine = QAEngine(db)
search_index = SearchIndex(db)

# Routes are defined as FastAPI endpoints
@app.post("/upload")
async def upload_files(...):
    # File upload logic
    pass

@app.post("/search")
async def search(...):
    # Search logic using FAISS
    pass
```

### Module Organization

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPI app + all endpoints
â”‚   â”œâ”€â”€ database.py                # SQLite/SQLModel backend
â”‚   â”œâ”€â”€ database_supabase.py       # Supabase backend
â”‚   â”œâ”€â”€ database_factory.py        # Backend factory pattern
â”‚   â”œâ”€â”€ qa_engine.py               # Question answering engine
â”‚   â”œâ”€â”€ search.py                  # FAISS vector search
â”‚   â”œâ”€â”€ chr_pipeline.py            # CHR data structuring
â”‚   â”œâ”€â”€ export_poml.py             # POML generation
â”‚   â”œâ”€â”€ hrm.py                     # Experimental HRM
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/                 # Document processors
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py      # Docling PDF pipeline
â”‚   â”‚   â”œâ”€â”€ csv_processor.py      # CSV/spreadsheet
â”‚   â”‚   â”œâ”€â”€ xlsx_processor.py     # Excel processing
â”‚   â”‚   â”œâ”€â”€ xml_ingestion.py      # XML log parsing
â”‚   â”‚   â”œâ”€â”€ openapi_ingestion.py  # OpenAPI specs
â”‚   â”‚   â”œâ”€â”€ postman_ingestion.py  # Postman collections
â”‚   â”‚   â”œâ”€â”€ web_ingestion.py      # Web scraping
â”‚   â”‚   â”œâ”€â”€ media_transcriber.py  # Audio/video
â”‚   â”‚   â””â”€â”€ image_ocr.py          # Image OCR
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                  # Analysis modules
â”‚   â”‚   â”œâ”€â”€ financial_statement_detector.py
â”‚   â”‚   â”œâ”€â”€ structure_processor.py
â”‚   â”‚   â”œâ”€â”€ ner_processor.py
â”‚   â”‚   â”œâ”€â”€ metric_extractor.py
â”‚   â”‚   â””â”€â”€ summarization.py
â”‚   â”‚
â”‚   â””â”€â”€ extraction/
â”‚       â””â”€â”€ langextract_adapter.py # Tag extraction
â”‚
â”œâ”€â”€ migrations/                    # Alembic migrations
â”œâ”€â”€ tests/                         # Unit tests
â””â”€â”€ requirements.txt               # Dependencies
```

### Database Schema

```sql
-- Artifacts table
CREATE TABLE artifact (
    id VARCHAR PRIMARY KEY,
    filename VARCHAR NOT NULL,
    filepath VARCHAR NOT NULL,
    filetype VARCHAR NOT NULL,
    report_week VARCHAR,
    status VARCHAR,
    source_url VARCHAR,
    extra_json TEXT  -- JSON extras
);

-- Facts table
CREATE TABLE fact (
    id VARCHAR PRIMARY KEY,
    artifact_id VARCHAR REFERENCES artifact(id),
    page_number INTEGER,
    content TEXT,
    confidence FLOAT,
    report_week VARCHAR
);

-- Evidence table
CREATE TABLE evidence (
    id VARCHAR PRIMARY KEY,
    artifact_id VARCHAR REFERENCES artifact(id),
    content_type VARCHAR,  -- table, chart, formula, etc.
    locator VARCHAR,       -- page/location
    preview TEXT,          -- short preview
    full_data_json TEXT    -- complete JSON data
);

-- Log entries
CREATE TABLE logrow (
    id VARCHAR PRIMARY KEY,
    artifact_id VARCHAR REFERENCES artifact(id),
    ts VARCHAR,            -- timestamp
    level VARCHAR,         -- ERROR, WARN, INFO
    code VARCHAR,          -- error code
    component VARCHAR,     -- service name
    message TEXT
);

-- API operations
CREATE TABLE apirow (
    id VARCHAR PRIMARY KEY,
    artifact_id VARCHAR REFERENCES artifact(id),
    path VARCHAR,
    method VARCHAR,        -- GET, POST, etc.
    operation_id VARCHAR,
    summary TEXT,
    details_json TEXT
);

-- Tags
CREATE TABLE tagrow (
    id VARCHAR PRIMARY KEY,
    artifact_id VARCHAR REFERENCES artifact(id),
    document_id VARCHAR,
    name VARCHAR,
    category VARCHAR,
    confidence FLOAT,
    hrm_steps INTEGER,     -- if HRM used
    hrm_metadata_json TEXT
);

-- Summaries
CREATE TABLE summaryrow (
    id VARCHAR PRIMARY KEY,
    style VARCHAR,         -- bullet, executive, action_items
    scope VARCHAR,         -- workspace, artifact
    scope_json TEXT,       -- details
    summary TEXT,
    created_at VARCHAR
);
```

### Database Factory Pattern

```python
# app/database_factory.py

def init_database():
    """Initialize database backend based on config."""
    backend = os.getenv("DB_BACKEND", "sqlite")

    if backend == "supabase":
        from app.database_supabase import SupabaseDatabase
        db = SupabaseDatabase(...)
        return db, "supabase"

    elif backend == "sqlite":
        from app.database import ExtendedDatabase
        db = ExtendedDatabase("database.db")
        return db, "sqlite"

    # Dual-write mode for migration
    if os.getenv("SUPABASE_DUAL_WRITE") == "true":
        from app.database import ExtendedDatabase
        from app.database_supabase import SupabaseDatabase

        sqlite_db = ExtendedDatabase("database.db")
        supabase_db = SupabaseDatabase(...)

        # Wrapper that writes to both
        class DualWriteDB:
            def add_artifact(self, data):
                sqlite_db.add_artifact(data)
                supabase_db.add_artifact(data)
                return data["id"]
            # ... other methods

        return DualWriteDB(), "dual"
```

---

## Frontend Architecture

### Next.js Structure

```
frontend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.tsx          # Root layout
â”‚   â”œâ”€â”€ page.tsx            # Main SPA
â”‚   â””â”€â”€ globals.css         # Global styles
â”‚
â”œâ”€â”€ components/             # React components
â”‚   â”œâ”€â”€ HeaderBar.tsx       # Top navigation
â”‚   â”œâ”€â”€ GlobalSearch.tsx    # Search bar
â”‚   â”œâ”€â”€ FileUpload.tsx      # Drag & drop upload
â”‚   â”œâ”€â”€ QAInterface.tsx     # Q&A tab
â”‚   â”œâ”€â”€ FactsViewer.tsx     # Facts display
â”‚   â”œâ”€â”€ TagsPanel.tsx       # Tag extraction UI
â”‚   â”œâ”€â”€ LogsPanel.tsx       # Logs viewer
â”‚   â”œâ”€â”€ APIsPanel.tsx       # API catalog
â”‚   â”œâ”€â”€ ArtifactsPanel.tsx  # Artifact list
â”‚   â”œâ”€â”€ EntitiesPanel.tsx   # NER results
â”‚   â”œâ”€â”€ StructurePanel.tsx  # Document structure
â”‚   â”œâ”€â”€ MetricHitsPanel.tsx # Metrics display
â”‚   â”œâ”€â”€ SummariesPanel.tsx  # Summaries
â”‚   â”œâ”€â”€ MediaArtifactsPanel.tsx  # Audio/video/OCR
â”‚   â”œâ”€â”€ CHRPanel.tsx        # CHR interface
â”‚   â”œâ”€â”€ SettingsModal.tsx   # Settings
â”‚   â””â”€â”€ Toast.tsx           # Notifications
â”‚
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ config.ts           # Configuration helpers
â”‚
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ next.config.js
â””â”€â”€ tailwind.config.js
```

### Component Architecture

```typescript
// Main page - Tab-based UI
export default function Home() {
  const [activeTab, setActiveTab] = useState('upload');

  return (
    <div>
      <HeaderBar />
      <TabNavigation activeTab={activeTab} onChange={setActiveTab} />

      {activeTab === 'upload' && <FileUpload />}
      {activeTab === 'facts' && <FactsViewer />}
      {activeTab === 'qa' && <QAInterface />}
      {activeTab === 'tags' && <TagsPanel />}
      {/* ... more tabs */}
    </div>
  );
}

// Component pattern
function FactsViewer() {
  const [facts, setFacts] = useState([]);
  const API = getApiBase();

  useEffect(() => {
    fetch(`${API}/facts`)
      .then(r => r.json())
      .then(data => setFacts(data.facts));
  }, []);

  return (
    <div>
      {facts.map(fact => (
        <FactCard key={fact.id} fact={fact} />
      ))}
    </div>
  );
}
```

---

## Data Flow

### Upload & Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚â”€â”€â”€â”€â–¶â”‚ Frontend â”‚â”€â”€â”€â”€â–¶â”‚  FastAPI â”‚â”€â”€â”€â”€â–¶â”‚   File   â”‚
â”‚         â”‚     â”‚  (Next)  â”‚     â”‚ /upload  â”‚     â”‚  Storage â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Process by type   â”‚
                              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                              â”‚ PDF â†’ Docling     â”‚
                              â”‚ CSV â†’ pandas      â”‚
                              â”‚ XML â†’ XPath       â”‚
                              â”‚ JSON â†’ OpenAPI    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Extract Facts &   â”‚
                              â”‚ Evidence          â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Store in Database â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Update FAISS Indexâ”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Search Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User   â”‚â”€â”€â”€â”€â–¶â”‚  Search  â”‚â”€â”€â”€â”€â–¶â”‚ Embeddingâ”‚
â”‚  Query  â”‚     â”‚   API    â”‚     â”‚  Model   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Query Vector      â”‚
                              â”‚ (384 dimensions)  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ FAISS Search      â”‚
                              â”‚ (cosine similarity)â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Top-K Results     â”‚
                              â”‚ with scores       â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Filter by type    â”‚
                              â”‚ (if specified)    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Return to Client  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Q&A Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Question â”‚â”€â”€â”€â”€â–¶â”‚ Q&A API  â”‚â”€â”€â”€â”€â–¶â”‚  Search  â”‚
â”‚         â”‚     â”‚          â”‚     â”‚  Facts   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Top-10 Candidates â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Rank by Relevance â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Extract Answer    â”‚
                              â”‚ (simple string    â”‚
                              â”‚  matching/summary)â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Optional: HRM     â”‚
                              â”‚ Refinement        â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Return Answer +   â”‚
                              â”‚ Citations         â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Processing Pipeline

### PDF Processing (Docling)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PDF Upload  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docling Converter   â”‚
â”‚ - Layout Analysis   â”‚
â”‚ - OCR (if needed)   â”‚
â”‚ - Table Detection   â”‚
â”‚ - Chart Extraction  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼          â–¼          â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tables  â”‚ â”‚ Charts â”‚ â”‚ Formulasâ”‚ â”‚  Text  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
      â”‚           â”‚          â”‚          â”‚
      â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
      â”‚   â”‚ Merge Multi-Page Tables          â”‚
      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                  â”‚
      â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Financial Statement Detection        â”‚
â”‚ - Income Statement (revenue/expense) â”‚
â”‚ - Balance Sheet (assets/liabilities) â”‚
â”‚ - Cash Flow (operating/investing)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Store as Evidence                    â”‚
â”‚ - Tables â†’ full_data JSON            â”‚
â”‚ - Charts â†’ image + caption           â”‚
â”‚ - Formulas â†’ LaTeX representation    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CSV/XLSX Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CSV Upload  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ pandas.read_csv()   â”‚
â”‚ - Detect headers    â”‚
â”‚ - Infer types       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric Extraction   â”‚
â”‚ - revenue, clicks   â”‚
â”‚ - impressions       â”‚
â”‚ - CTR calculation   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create Facts        â”‚
â”‚ - 1 fact per row    â”‚
â”‚ - Include metrics   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Store in Database   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### XML Log Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ XML Upload  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apply XPath Mapping â”‚
â”‚ - entry path        â”‚
â”‚ - field mappings    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parse Each Entry    â”‚
â”‚ - timestamp         â”‚
â”‚ - level (severity)  â”‚
â”‚ - code (error code) â”‚
â”‚ - component         â”‚
â”‚ - message           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Store as LogRow     â”‚
â”‚ - Indexed by level  â”‚
â”‚ - Queryable fields  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Storage Architecture

### File System Layout

```
PMOVES-DoX/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ database.db              # SQLite database
â”‚   â”‚
â”‚   â”œâ”€â”€ uploads/                 # Uploaded files
â”‚   â”‚   â”œâ”€â”€ uuid1_document.pdf
â”‚   â”‚   â”œâ”€â”€ uuid2_data.csv
â”‚   â”‚   â””â”€â”€ uuid3_logs.xml
â”‚   â”‚
â”‚   â”œâ”€â”€ artifacts/               # Processed outputs
â”‚   â”‚   â”œâ”€â”€ charts/              # Extracted charts
â”‚   â”‚   â”‚   â”œâ”€â”€ uuid1_chart1.png
â”‚   â”‚   â”‚   â””â”€â”€ uuid1_chart2.png
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ web/                 # Web page artifacts
â”‚   â”‚   â”‚   â”œâ”€â”€ uuid_web.html
â”‚   â”‚   â”‚   â”œâ”€â”€ uuid_web.txt
â”‚   â”‚   â”‚   â””â”€â”€ uuid_web.metadata.json
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ chr_clusters.csv     # CHR output
â”‚   â”‚   â”œâ”€â”€ chr_pca.png          # PCA visualization
â”‚   â”‚   â”œâ”€â”€ datavzrd.yaml        # Dashboard config
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ poml/                # POML exports
â”‚   â”‚       â””â”€â”€ uuid.poml
â”‚   â”‚
â”‚   â””â”€â”€ .faiss_index/            # FAISS index files
â”‚       â”œâ”€â”€ index.bin
â”‚       â””â”€â”€ metadata.json
```

### FAISS Index Structure

```python
# app/search.py

class SearchIndex:
    def __init__(self, db):
        self.db = db
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.index = None  # FAISS index
        self.fact_ids = [] # Corresponding fact IDs

    def rebuild(self):
        """Rebuild index from all facts."""
        facts = self.db.get_facts()

        # Generate embeddings
        texts = [f['content'] for f in facts]
        embeddings = self.model.encode(texts)  # (N, 384)

        # Create FAISS index
        dimension = 384
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings.astype('float32'))

        # Store mapping
        self.fact_ids = [f['id'] for f in facts]

    def search(self, query, k=10):
        """Search for top-k similar facts."""
        # Embed query
        query_vec = self.model.encode([query])

        # Search FAISS
        distances, indices = self.index.search(
            query_vec.astype('float32'),
            k
        )

        # Map back to facts
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            fact_id = self.fact_ids[idx]
            fact = self.db.get_fact(fact_id)
            results.append({
                'fact': fact,
                'score': 1 / (1 + dist),  # Convert distance to similarity
                'distance': dist
            })

        return results
```

---

## Search & Indexing

### Embedding Model

```
Model: all-MiniLM-L6-v2
Dimensions: 384
Provider: sentence-transformers
Max sequence: 256 tokens

Why this model?
- Fast (CPU-friendly)
- Multilingual support
- Good balance of quality/performance
- Small model size (~80MB)
```

### Index Management

```python
# Lifecycle
1. Initial build: On first search request
2. Incremental updates: After each upload
3. Full rebuild: Manual trigger via /search/rebuild

# Storage
- In-memory during runtime
- Persisted to disk (optional)
- Rebuilt from database on restart
```

### Search Optimization

```python
# 1. GPU Acceleration (optional)
if torch.cuda.is_available():
    model = model.to('cuda')
    index = faiss.index_cpu_to_gpu(resource, 0, index)

# 2. Approximate Search (for large datasets)
# Use HNSW or IVF instead of FlatL2
index = faiss.IndexHNSWFlat(dimension, 32)
index.hnsw.efConstruction = 40
index.hnsw.efSearch = 16

# 3. Dimensionality Reduction
# Use PCA to reduce from 384 â†’ 128 dimensions
pca = faiss.PCAMatrix(384, 128)
index = faiss.IndexPreTransform(pca, faiss.IndexFlatL2(128))
```

---

## Security Architecture

### Current Security Model

**Authentication:** None (local use)
**Authorization:** None
**Data Access:** Full read/write

### Security Features Implemented

1. **Path Traversal Protection**
```python
# Sanitize filenames
safe_filename = os.path.basename(file.filename)
safe_filename = safe_filename.replace("/", "_").replace("\\", "_")
```

2. **SSRF Protection**
```python
# Block private IPs
ip = ipaddress.ip_address(hostname)
if ip.is_private or ip.is_loopback:
    raise ValueError("Access denied")
```

3. **File Size Limits**
```python
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
if len(file_content) > MAX_FILE_SIZE:
    raise HTTPException(413, "File too large")
```

4. **CORS Restrictions**
```python
# Only allow configured origins
allow_origins = [os.getenv("FRONTEND_ORIGIN")]
```

### Production Security Recommendations

```
1. Add Authentication
   - Use OAuth2/JWT
   - Implement API keys
   - Add rate limiting

2. Network Security
   - Deploy behind VPN
   - Use reverse proxy (nginx)
   - Enable HTTPS/TLS

3. Data Security
   - Encrypt database
   - Encrypt uploaded files
   - Implement audit logging

4. Access Control
   - Role-based permissions
   - Document-level access control
   - Multi-tenancy support
```

---

## Deployment Models

### Development (Local)

```bash
# Backend
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload --host 0.0.0.0 --port 8484

# Frontend
cd frontend
npm install
npm run dev
```

### Docker Compose (CPU)

```yaml
# docker-compose.cpu.yml
services:
  backend:
    build: ./backend
    ports:
      - "8484:8484"
    volumes:
      - ./backend/uploads:/app/uploads
      - ./backend/artifacts:/app/artifacts

  frontend:
    build: ./frontend
    ports:
      - "3737:3737"
    depends_on:
      - backend
```

### Docker Compose (GPU)

```yaml
# docker-compose.yml
services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.gpu
    ports:
      - "8484:8484"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

### Kubernetes (Production)

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pmoves-dox-backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pmoves-dox-backend
  template:
    metadata:
      labels:
        app: pmoves-dox-backend
    spec:
      containers:
      - name: backend
        image: pmoves-dox:latest
        ports:
        - containerPort: 8484
        env:
        - name: DB_BACKEND
          value: "supabase"
        resources:
          limits:
            nvidia.com/gpu: 1
```

### Cloud Deployment (AWS)

```
Architecture:
- ECS Fargate (backend containers)
- S3 (file storage)
- RDS PostgreSQL (database)
- CloudFront (frontend)
- API Gateway (rate limiting)
- Cognito (authentication)

Benefits:
- Auto-scaling
- Managed services
- High availability
- Global CDN
```

---

## Performance Considerations

### Bottlenecks

1. **PDF Processing**
   - Docling is CPU/GPU intensive
   - Large PDFs take minutes
   - Solution: Async processing, queue

2. **FAISS Search**
   - Linear search O(N)
   - Solution: Use HNSW approximation

3. **Database Queries**
   - SQLite limitations for concurrent writes
   - Solution: Migrate to PostgreSQL/Supabase

### Optimization Strategies

```python
# 1. Caching
from functools import lru_cache

@lru_cache(maxsize=100)
def get_artifact(artifact_id):
    return db.get_artifact(artifact_id)

# 2. Pagination
@app.get("/facts")
def get_facts(offset: int = 0, limit: int = 100):
    return db.get_facts(offset, limit)

# 3. Background Tasks
from fastapi import BackgroundTasks

@app.post("/upload")
async def upload(file, background_tasks: BackgroundTasks):
    background_tasks.add_task(process_pdf, file)
    return {"status": "queued"}

# 4. Connection Pooling
from sqlalchemy.pool import QueuePool

engine = create_engine(
    "sqlite:///database.db",
    poolclass=QueuePool,
    pool_size=10
)
```

---

## Monitoring & Observability

### Metrics Endpoint

```http
GET /metrics
```

Returns Prometheus-formatted metrics:
```
# HELP pmoves_requests_total Total requests
# TYPE pmoves_requests_total counter
pmoves_requests_total{endpoint="/upload",status="200"} 42
pmoves_requests_total{endpoint="/search",status="200"} 158

# HELP pmoves_processing_time_seconds Processing time
# TYPE pmoves_processing_time_seconds histogram
pmoves_processing_time_seconds_bucket{le="1.0"} 45
pmoves_processing_time_seconds_bucket{le="5.0"} 89
```

### Logging

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.post("/upload")
async def upload(...):
    logger.info(f"Upload started: {file.filename}")
    # Process file
    logger.info(f"Upload completed: {artifact_id}")
```

### Health Checks

```python
@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "database": "connected" if db.test_connection() else "error",
        "faiss": "loaded" if search_index.index else "not_loaded"
    }
```

---

## Next Steps

- ğŸ“– Review [USER_GUIDE.md](./USER_GUIDE.md) for usage
- ğŸ³ Try [COOKBOOKS.md](./COOKBOOKS.md) for examples
- ğŸ”§ Check [API_REFERENCE.md](./API_REFERENCE.md) for API docs

**Questions?** Open an issue on GitHub!
