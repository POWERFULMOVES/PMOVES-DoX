# -*- coding: utf-8 -*-
"""Any_Format_Goes_Toy_MVP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a15nX730MpWU63t8KfJmIKrhhsLI6OVI

# Any-Format Goes — Weekly Analyst (Toy MVP, Stakeholder Demo)

This notebook is a **toy demonstration** of the idea: a friendly, **zero‑config** MVP that illustrates **what a production system could do** — without requiring messy installs or complex setup.

**What it shows (conceptually):**
1. A stakeholder drops **any weekly files** (we simulate this with built‑in sample files).
2. The system **normalizes** data into a simple **MeetingFacts** store with **evidence pointers**.
3. You can **ask questions** like *"what is roas"* and get **answers with citations**.
4. A tiny **dashboard UI** lets you show this flow live.

> This toy demo uses synthetic data and simple HTML parsing placeholders instead of full OCR/layout parsing. The goal is to **communicate the vision**, not to ship production code.

## 1) Lightweight setup
"""

# Standard libs only — keeps things simple for stakeholders.
import os, json, re, uuid, textwrap, random
from pathlib import Path
from typing import List, Dict, Any, Optional

import pandas as pd

ROOT = Path.cwd()
SAMPLES = ROOT / "toy_samples"
ARTIFACTS_DIR = ROOT / "toy_artifacts"
for p in [SAMPLES, ARTIFACTS_DIR]:
    p.mkdir(exist_ok=True)

def clean(s: str) -> str:
    return re.sub(r'[^A-Za-z0-9_.-]+','_', s)

"""## 2) Create embedded sample weekly files (simulated uploads)"""

# We'll generate a few realistic-looking weekly files:
# - CSV ad export
# - XLSX revenue summary
# - "PDF export" simulated via pre-rendered HTML (toy stand-in for parsed PDF)
#
# Stakeholders don't have to upload anything — just run the notebook.

# 2.1 CSV: ads_export.csv
ads_csv_path = SAMPLES / "ads_export.csv"
ads_df = pd.DataFrame({
    "Campaign": ["Brand - US - Search", "Brand - US - Display", "Brand - CA - Search"],
    "Spend": [8450.50, 2400.00, 1600.00],
    "Clicks": [12500, 2100, 1500],
    "Impressions": [380000, 92000, 54000],
    "Conversions": [420, 36, 29],
    "Revenue": [28500.00, 3100.00, 4200.00]
})
ads_df.to_csv(ads_csv_path, index=False)

# 2.2 XLSX: revenue_summary.xlsx
rev_xlsx_path = SAMPLES / "revenue_summary.xlsx"
rev_df = pd.DataFrame({
    "Segment": ["US", "CA", "MX"],
    "Revenue": [32000.00, 4200.00, 0.00],
    "Leads": [520, 31, 0]
})
with pd.ExcelWriter(rev_xlsx_path) as xw:
    rev_df.to_excel(xw, index=False, sheet_name="Summary")
    # add a second sheet to look realistic
    pd.DataFrame({
        "Source": ["Search", "Display", "Social"],
        "Revenue": [24000, 5000, 7200]
    }).to_excel(xw, index=False, sheet_name="ByChannel")

# 2.3 "PDF": performance_report.html (toy stand-in for PDF parsing)
# In production we’d parse a PDF -> structured HTML (Docling). Here we just write HTML directly.
pdf_html_path = SAMPLES / "performance_report.html"
pdf_html = '''
<html><body>
  <h2>Slide 5: Weekly Performance</h2>
  <table border="1">
    <tr><th>Metric</th><th>Value</th></tr>
    <tr><td>CPA</td><td>$ 18.45</td></tr>
    <tr><td>ROAS</td><td>3.27 x</td></tr>
    <tr><td>CTR</td><td>3.1 %</td></tr>
  </table>
  <p>Note: ROAS softness driven by mix shift to Display.</p>
</body></html>
'''.strip()
pdf_html_path.write_text(pdf_html, encoding="utf-8")

print("Sample files created:")
print("-", ads_csv_path.name)
print("-", rev_xlsx_path.name)
print("-", pdf_html_path.name)

"""## 3) Ingestion (toy): normalize mixed files into MeetingFacts with evidence"""

# We'll keep a minimal MeetingFacts & Evidence store in memory.
MEETING_FACTS: List[Dict[str, Any]] = []
EVIDENCE: List[Dict[str, Any]] = []
ARTIFACTS: List[Dict[str, Any]] = []

def add_evidence(artifact_id: str, locator: str, preview: str, file_path: str) -> str:
    evid = {
        "id": str(uuid.uuid4()),
        "artifact_id": artifact_id,
        "locator": locator,
        "preview": preview[:300],
        "file_path": file_path
    }
    EVIDENCE.append(evid)
    return evid["id"]

def ingest_csv(path: Path, report_week: str):
    artifact_id = str(uuid.uuid4())
    df = pd.read_csv(path)
    ARTIFACTS.append({"id": artifact_id, "kind": "csv", "path": str(path)})
    # Compute totals & derived metrics
    totals = {
        "spend": float(df["Spend"].sum()),
        "clicks": float(df["Clicks"].sum()),
        "impressions": float(df["Impressions"].sum()),
        "conversions": float(df["Conversions"].sum()),
        "revenue": float(df["Revenue"].sum())
    }
    totals["ctr"] = totals["clicks"] / max(1.0, totals["impressions"])
    totals["cpa"] = totals["spend"] / max(1.0, totals["conversions"])
    totals["roas"] = totals["revenue"] / max(1.0, totals["spend"])
    evid = add_evidence(artifact_id, locator=f"{path.name}#table0", preview=str(df.head(5)), file_path=str(path))
    MEETING_FACTS.append({
        "id": str(uuid.uuid4()), "report_week": report_week,
        "entity": None, "metrics": totals, "evidence_id": evid
    })

def ingest_xlsx(path: Path, report_week: str):
    artifact_id = str(uuid.uuid4())
    xls = pd.ExcelFile(path)
    ARTIFACTS.append({"id": artifact_id, "kind": "xlsx", "path": str(path), "sheets": xls.sheet_names})
    # Use the Summary sheet for a toy total
    df = xls.parse("Summary")
    totals = {
        "revenue": float(df["Revenue"].sum()),
        "conversions": float(df["Leads"].sum())
    }
    evid = add_evidence(artifact_id, locator=f"{path.name}#Summary", preview=str(df.head(5)), file_path=str(path))
    MEETING_FACTS.append({
        "id": str(uuid.uuid4()), "report_week": report_week,
        "entity": None, "metrics": totals, "evidence_id": evid
    })

def ingest_html_report(path: Path, report_week: str):
    # Toy parser: look for simple key metrics in the HTML text
    artifact_id = str(uuid.uuid4())
    html = path.read_text(encoding="utf-8", errors="ignore")
    ARTIFACTS.append({"id": artifact_id, "kind": "html_report", "path": str(path)})
    # crude extractions
    def find_num(tag):
        m = re.search(tag + r"[^0-9]*([0-9]+(?:\.[0-9]+)?)", html, re.I)
        return float(m.group(1)) if m else None
    cpa = find_num("CPA"); roas = find_num("ROAS"); ctr = find_num("CTR")
    metrics = {}
    if cpa is not None: metrics["cpa"] = cpa
    if roas is not None: metrics["roas"] = roas
    if ctr is not None: metrics["ctr"] = ctr/100.0 if ctr > 1.0 else ctr  # convert % if needed
    evid = add_evidence(artifact_id, locator=f"{path.name}#table0", preview=html[:250], file_path=str(path))
    MEETING_FACTS.append({
        "id": str(uuid.uuid4()), "report_week": report_week,
        "entity": None, "metrics": metrics, "evidence_id": evid
    })

def run_toy_ingest(report_week: str = "2025-09-22..2025-09-28"):
    MEETING_FACTS.clear(); EVIDENCE.clear(); ARTIFACTS.clear()
    ingest_csv(Path(ads_csv_path), report_week)
    ingest_xlsx(Path(rev_xlsx_path), report_week)
    ingest_html_report(Path(pdf_html_path), report_week)

run_toy_ingest()
print("Artifacts:", len(ARTIFACTS), "| Facts:", len(MEETING_FACTS), "| Evidence:", len(EVIDENCE))

"""## 4) Show MeetingFacts and Evidence (what the bot would use)"""

pd.set_option('display.max_colwidth', 160)
facts_df = pd.DataFrame(MEETING_FACTS)
evid_df = pd.DataFrame(EVIDENCE)
facts_df_display = facts_df[["id","report_week","metrics","evidence_id"]]
evid_df_display = evid_df[["id","artifact_id","locator","preview"]]
facts_df_display, evid_df_display

"""## 5) Ask questions — with citations (toy logic)"""

def ask(q: str) -> Dict[str, Any]:
    ql = q.lower()
    metric_map = {
        "spend":"spend","clicks":"clicks","impressions":"impressions",
        "conversions":"conversions","revenue":"revenue",
        "cpa":"cpa","roas":"roas","ctr":"ctr"
    }
    m = None
    for k in metric_map:
        if re.search(rf"\b{re.escape(k)}\b", ql):
            m = metric_map[k]; break
    if not m:
        return {"answer": "This toy demo recognizes basic metrics like spend, revenue, roas, cpa, ctr.", "evidence": []}
    vals, evids = [], []
    for f in MEETING_FACTS:
        v = f["metrics"].get(m)
        if isinstance(v,(int,float)):
            vals.append(v); evids.append(f["evidence_id"])
    if not vals:
        return {"answer": f"No '{m}' found in the sample data.", "evidence": []}
    total = sum(vals)
    return {"answer": f"{m.upper()} total (toy demo across sources): {total:,.4f}", "evidence": evids}

# Example
ask("what is roas")

"""## 6) Tiny stakeholder dashboard (optional)"""

# Gradio is lightweight and usually works in Colab.
try:
    import gradio as gr

    def ui_reset():
        run_toy_ingest()
        return "Reset with built-in sample files.", pd.DataFrame(MEETING_FACTS), pd.DataFrame(EVIDENCE)

    def ui_ask(q):
        res = ask(q)
        rows = []
        for eid in res.get("evidence", []):
            e = next((x for x in EVIDENCE if x["id"] == eid), None)
            if e:
                rows.append([eid, e["locator"], e["preview"][:160]])
        return res["answer"], rows

    with gr.Blocks(title="Any-Format Goes — Toy MVP") as demo:
        gr.Markdown("**Built-in sample files are already ingested.** Ask a question or click reset to re-run.")
        with gr.Row():
            reset_btn = gr.Button("Reset (reload samples)")
        facts = gr.Dataframe(value=pd.DataFrame(MEETING_FACTS), label="MeetingFacts (toy)")
        evid = gr.Dataframe(value=pd.DataFrame(EVIDENCE), label="Evidence (toy)")
        reset_btn.click(fn=ui_reset, outputs=[gr.Textbox(label="Status"), facts, evid])

        gr.Markdown("---")
        q = gr.Textbox(label="Ask a question, e.g. 'what is roas', 'total spend', 'ctr'")
        ask_btn = gr.Button("Ask")
        answer_box = gr.Textbox(label="Answer")
        cited_tbl = gr.Dataframe(headers=["evidence_id","locator","preview_snippet"], label="Citations")
        ask_btn.click(fn=ui_ask, inputs=[q], outputs=[answer_box, cited_tbl])

    demo.launch(quiet=True)
except Exception as e:
    print("Gradio not available or failed to start (ok for toy demo). Error:", e)

"""## 7) Stakeholder talking points (what a full build adds)

- **Real Any-Format Ingestion:** swap the toy HTML/CSV readers with a production parser stack (e.g., Docling + Granite-Docling) for OCR, layout, tables, charts, formulas, and code.
- **Entity-Aware Facts:** per-campaign / channel / geo segmentation; WoW/MoM rollups; anomaly flags.
- **Provenance Everywhere:** page/region/table cell pointers so the bot can **show the exact source** live in Zoom.
- **Deterministic Math:** all calculations in SQL/DuckDB/Pandas; the LLM only orchestrates tool calls and narration.
- **Zoom Participant:** low-latency ASR/TTS; agenda running; “show sources” button in chat; safe modes (Observer/Facilitator).
- **Security & Comms:** RBAC, per-client silo, consent banner, audit logs, redaction commands.

## 8) How to demo in 60 seconds

1. Run all cells.
2. Scroll to **MeetingFacts** to show auto totals (ROAS/CPA/CTR derived).
3. Use the **dashboard** (if it launched) and ask “what is roas” → see answer + citations.
4. Explain how a production build swaps toy parsers for **Docling** and adds Zoom + precompute.
"""